{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lime\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 종속 패키지 설차\n",
    "# !pip install requests\n",
    "# !pip install tabulate\n",
    "# !pip install \"colorama>=0.3.8\"\n",
    "# !pip install future\n",
    "# # 기존 h2o 삭제\n",
    "# !pip uninstall h2o\n",
    "# # h2o 설치\n",
    "# !pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import h2o\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n"
     ]
    },
    {
     "ename": "H2OStartupError",
     "evalue": "Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/h2o.py:270\u001b[0m, in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     h2oconn \u001b[38;5;241m=\u001b[39m \u001b[43mH2OConnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mverify_ssl_certificates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_ssl_certificates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcacert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcacert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmsgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChecking whether there is an H2O instance running at \u001b[39;49m\u001b[38;5;132;43;01m{url}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconnected.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnot found.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mstrict_version_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m H2OConnectionError:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# Backward compatibility: in init() port parameter really meant \"baseport\" when starting a local server...\u001b[39;00m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/backend/connection.py:386\u001b[0m, in \u001b[0;36mH2OConnection.open\u001b[0;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, msgs, strict_version_check)\u001b[0m\n\u001b[1;32m    385\u001b[0m conn\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.0\u001b[39m\n\u001b[0;32m--> 386\u001b[0m conn\u001b[38;5;241m.\u001b[39m_cluster \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[39;00m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/backend/connection.py:690\u001b[0m, in \u001b[0;36mH2OConnection._test_connection\u001b[0;34m(self, max_retries, messages)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m H2OConnectionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not establish link to the H2O cloud \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m retries\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    691\u001b[0m                              \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_url, max_retries, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)))\n",
      "\u001b[0;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[01:10.01] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f69470bfd00>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[01:10.21] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f69470e6e20>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[01:10.41] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6947087520>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[01:10.62] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6947087d30>: Failed to establish a new connection: [Errno 111] Connection refused'))\n[01:10.82] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f69470879d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mH2OStartupError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start an H2O virtual cluster that uses 6 gigs of RAM and 6 cores\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mh2o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_mem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m500M\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthreads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Clean up the cluster just in case\u001b[39;00m\n\u001b[1;32m      5\u001b[0m h2o\u001b[38;5;241m.\u001b[39mremove_all()\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/h2o.py:287\u001b[0m, in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m https:\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m H2OConnectionError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting local server is not available with https enabled. You may start local\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    285\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instance of H2O with https manually \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    286\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#new-user-quick-start).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m     hs \u001b[38;5;241m=\u001b[39m \u001b[43mH2OLocalServer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_assertions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_assertions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mem_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmin_mem_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mice_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mice_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmax_log_file_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_file_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mextra_classpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_classpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvm_custom_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjvm_custom_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbind_to_localhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbind_to_localhost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     h2oconn \u001b[38;5;241m=\u001b[39m H2OConnection\u001b[38;5;241m.\u001b[39mopen(server\u001b[38;5;241m=\u001b[39mhs, https\u001b[38;5;241m=\u001b[39mhttps, verify_ssl_certificates\u001b[38;5;241m=\u001b[39mverify_ssl_certificates,\n\u001b[1;32m    293\u001b[0m                                  cacert\u001b[38;5;241m=\u001b[39mcacert, auth\u001b[38;5;241m=\u001b[39mauth, proxy\u001b[38;5;241m=\u001b[39mproxy, cookies\u001b[38;5;241m=\u001b[39mcookies, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    294\u001b[0m                                  strict_version_check\u001b[38;5;241m=\u001b[39msvc)\n\u001b[1;32m    295\u001b[0m h2oconn\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mtimezone \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/backend/server.py:140\u001b[0m, in \u001b[0;36mH2OLocalServer.start\u001b[0;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[1;32m    137\u001b[0m     hs\u001b[38;5;241m.\u001b[39m_tempdir \u001b[38;5;241m=\u001b[39m hs\u001b[38;5;241m.\u001b[39m_ice_root\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to start a local H2O server...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m \u001b[43mhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnthreads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mea\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_assertions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_mem_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_mem_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjvm_custom_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjvm_custom_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbind_to_localhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbind_to_localhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_log_file_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_file_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Server is running at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m://\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (hs\u001b[38;5;241m.\u001b[39mscheme, hs\u001b[38;5;241m.\u001b[39mip, hs\u001b[38;5;241m.\u001b[39mport))\n\u001b[1;32m    144\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28;01mlambda\u001b[39;00m: hs\u001b[38;5;241m.\u001b[39mshutdown())\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/backend/server.py:272\u001b[0m, in \u001b[0;36mH2OLocalServer._launch_server\u001b[0;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Find Java and check version. (Note that subprocess.check_output returns the output as a bytes object)\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m java \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_java(java, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/opt/software/lib/python3.9/site-packages/h2o/backend/server.py:444\u001b[0m, in \u001b[0;36mH2OLocalServer._find_java\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, java)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# not found...\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m H2OStartupError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find Java. Please install the latest JRE from\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mH2OStartupError\u001b[0m: Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements"
     ]
    }
   ],
   "source": [
    "# Start an H2O virtual cluster that uses 6 gigs of RAM and 6 cores\n",
    "h2o.init(max_mem_size = \"500M\", nthreads = 6) \n",
    "\n",
    "# Clean up the cluster just in case\n",
    "h2o.remove_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper class\n",
    "\n",
    "We need a wrapper class that makes an H2O distributed random forest behave like a scikit learn random forest. We instantiate the class with an h2o distributed random forest object and column names. The predict_proba method takes a numpy array as input and returns an array of predicted probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class h2o_predict_proba_wrapper:\n",
    "    # drf is the h2o distributed random forest object, the column_names is the\n",
    "    # labels of the X values\n",
    "    def __init__(self,model,column_names):\n",
    "            \n",
    "            self.model = model\n",
    "            self.column_names = column_names\n",
    " \n",
    "    def predict_proba(self,this_array):        \n",
    "        # If we have just 1 row of data we need to reshape it\n",
    "        shape_tuple = np.shape(this_array)        \n",
    "        if len(shape_tuple) == 1:\n",
    "            this_array = this_array.reshape(1, -1)\n",
    "            \n",
    "        # We convert the numpy array that Lime sends to a pandas dataframe and\n",
    "        # convert the pandas dataframe to an h2o frame\n",
    "        self.pandas_df = pd.DataFrame(data = this_array,columns = self.column_names)\n",
    "        self.h2o_df = h2o.H2OFrame(self.pandas_df)\n",
    "        \n",
    "        # Predict with the h2o drf\n",
    "        self.predictions = self.model.predict(self.h2o_df).as_data_frame()\n",
    "        # the first column is the class labels, the rest are probabilities for\n",
    "        # each class\n",
    "        self.predictions = self.predictions.iloc[:,1:].as_matrix()\n",
    "        return self.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we'll use the Iris dataset, and we'll replace the scikit learn random forest with an h2o distributed random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "\n",
    "# Get the text names for the features and the class labels\n",
    "feature_names = iris.feature_names\n",
    "class_labels = 'species'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a train test split and convert to pandas and h2o frames\n",
    "\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(iris.data, iris.target, train_size=0.80)\n",
    "\n",
    "train_h2o_df = h2o.H2OFrame(train)\n",
    "train_h2o_df.set_names(iris.feature_names)\n",
    "train_h2o_df['species'] = h2o.H2OFrame(iris.target_names[labels_train])\n",
    "train_h2o_df['species'] = train_h2o_df['species'].asfactor()\n",
    "\n",
    "test_h2o_df = h2o.H2OFrame(test)\n",
    "test_h2o_df.set_names(iris.feature_names)\n",
    "test_h2o_df['species'] = h2o.H2OFrame(iris.target_names[labels_test])\n",
    "test_h2o_df['species'] = test_h2o_df['species'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "# rf.fit(train, labels_train)\n",
    "\n",
    "iris_drf = H2ORandomForestEstimator(\n",
    "        model_id=\"iris_drf\",\n",
    "        ntrees=500,\n",
    "        stopping_rounds=2,\n",
    "        score_each_iteration=True,\n",
    "        seed=1000000,\n",
    "        balance_classes=False,\n",
    "        histogram_type=\"AUTO\")\n",
    "\n",
    "iris_drf.train(x=feature_names,\n",
    "         y='species',\n",
    "         training_frame=train_h2o_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics.accuracy_score(labels_test, rf.predict(test))\n",
    "\n",
    "iris_drf.model_performance(test_h2o_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert h2o to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explainer requires numpy arrays as input and h2o requires the train and test data to be in h2o frames. In this case we could just use the train and test numpy arrays but for illustrative purposes here is how to convert an h2o frame to a pandas dataframe and a pandas dataframe to a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pandas_df = train_h2o_df[feature_names].as_data_frame() \n",
    "train_numpy_array = train_pandas_df.as_matrix() \n",
    "\n",
    "test_pandas_df = test_h2o_df[feature_names].as_data_frame() \n",
    "test_numpy_array = test_pandas_df.as_matrix() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to lime_text.TextExplainer, tabular explainers need a training set. The reason for this is because we compute statistics on each feature (column). If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. For this tutorial, we'll only look at numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these computed statistics for two things:\n",
    "1. To scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale\n",
    "2. To sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(train_numpy_array,\n",
    "                                                   feature_names=feature_names,\n",
    "                                                   class_names=iris.target_names,\n",
    "                                                   discretize_continuous=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the predictor wrapper instance for the h2o drf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a trained h2o distributed random forest that we would like to explain. We need to create a wrapper instance that will make it behave as a scikit learn random forest for our Lime explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_drf_wrapper = h2o_predict_proba_wrapper(iris_drf,feature_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining an instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multi-class classification problem, we set the top_labels parameter, so that we only explain the top class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = np.random.randint(0, test_numpy_array.shape[0])\n",
    "\n",
    "i = 27\n",
    "# exp = explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)\n",
    "\n",
    "exp = explainer.explain_instance(test_numpy_array[i], h2o_drf_wrapper.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explain a single instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is a lot going on here. First, note that the row we are explained is displayed on the right side, in table format. Since we had the show_all parameter set to false, only the features used in the explanation are displayed.\n",
    "\n",
    "The *value* column displays the original value for each feature.\n",
    "\n",
    "Note that LIME has discretized the features in the explanation. This is because we let discretize_continuous=True in the constructor (this is the default). Discretized features make for more intuitive explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the local linear approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = lambda x: iris.feature_names.index(x)\n",
    "\n",
    "print(test[i])\n",
    "print(test_numpy_array[i])\n",
    "\n",
    "# hide the parse and predict progress bars\n",
    "h2o.no_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Increasing petal width')\n",
    "temp = test_numpy_array[i].copy()\n",
    "print('P(setosa) before:', h2o_drf_wrapper.predict_proba(temp)[0,0])\n",
    "temp[feature_index('petal width (cm)')] = 1.5\n",
    "print('P(setosa) after:', h2o_drf_wrapper.predict_proba(temp)[0,0])\n",
    "print ()\n",
    "print('Increasing petal length')\n",
    "temp = test_numpy_array[i].copy()\n",
    "print('P(setosa) before:', h2o_drf_wrapper.predict_proba(temp)[0,0])\n",
    "temp[feature_index('petal length (cm)')] = 3.5\n",
    "print('P(setosa) after:', h2o_drf_wrapper.predict_proba(temp)[0,0])\n",
    "print()\n",
    "print('Increasing both')\n",
    "temp = test_numpy_array[i].copy()\n",
    "print('P(setosa) before:', h2o_drf_wrapper.predict_proba(temp)[0,0])\n",
    "temp[feature_index('petal width (cm)')] = 1.5\n",
    "temp[feature_index('petal length (cm)')] = 3.5\n",
    "print('P(setosa) after:', h2o_drf_wrapper.predict_proba(temp)[0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both features had the impact we thought they would. The scale at which they need to be perturbed of course depends on the scale of the feature in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show all features, just for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we will use the Mushroom dataset, which will be downloaded [here](http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data). The task is to predict if a mushroom is edible or poisonous, based on categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.genfromtxt('http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data', delimiter=',', dtype='<U20')\n",
    "data = np.genfromtxt('data/mushroom_data.csv', delimiter=',', dtype='<U20')\n",
    "labels = data[:,0]\n",
    "le= sklearn.preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "class_names = le.classes_\n",
    "data = data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = range(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = 'cap-shape,cap-surface,cap-color,bruises?,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring, stalk-surface-below-ring, stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat'.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expand the characters into words, using the data available in the UCI repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_names = '''bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s\n",
    "fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "bruises=t,no=f\n",
    "almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n",
    "attached=a,descending=d,free=f,notched=n\n",
    "close=c,crowded=w,distant=d\n",
    "broad=b,narrow=n\n",
    "black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "enlarging=e,tapering=t\n",
    "bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n",
    "fibrous=f,scaly=y,silky=k,smooth=s\n",
    "fibrous=f,scaly=y,silky=k,smooth=s\n",
    "brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "partial=p,universal=u\n",
    "brown=n,orange=o,white=w,yellow=y\n",
    "none=n,one=o,two=t\n",
    "cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d'''.split('\\n')\n",
    "for j, names in enumerate(categorical_names):\n",
    "    values = names.split(',')\n",
    "    values = dict([(x.split('=')[1], x.split('=')[0]) for x in values])\n",
    "    data[:,j] = np.array(list(map(lambda x: values[x], data[:,j])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our explainer (and most classifiers) takes in numerical data, even if the features are categorical. We thus transform all of the string attributes into integers, using sklearn's LabelEncoder. We use a dict to save the correspondence between the integer values and the original strings, so that we can present this later in the explanations.\n",
    "\n",
    "The h2o drf classifier can handle the original text, but we will convert to integers to Lime and then have h20 treat the categories as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_names = {}\n",
    "for feature in categorical_features:\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    le.fit(data[:, feature])\n",
    "    data[:, feature] = le.transform(data[:, feature])\n",
    "    categorical_names[feature] = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a train test split and convert to pandas and h2o frames\n",
    "\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n",
    "\n",
    "class_names=['edible', 'poisonous']\n",
    "\n",
    "train_h2o_df = h2o.H2OFrame(train)\n",
    "train_h2o_df.set_names(feature_names)\n",
    "train_h2o_df['class'] = h2o.H2OFrame(labels_train)\n",
    "train_h2o_df['class'] = train_h2o_df['class'].asfactor()\n",
    "\n",
    "test_h2o_df = h2o.H2OFrame(test)\n",
    "test_h2o_df.set_names(feature_names)\n",
    "test_h2o_df['class'] = h2o.H2OFrame(labels_test)\n",
    "test_h2o_df['class'] = test_h2o_df['class'].asfactor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Finally, we use a One-hot encoder, so that the classifier does not take our categorical features as continuous features. We will use this encoder only for the classifier, not for the explainer - and the reason is that the explainer must make sure that a categorical feature only has one value.~~\n",
    "\n",
    "A great feature of h2o is that it can handle categorical data directly. There is no need to one-hot encode. We will use the integer data for the explainer and have h2o treat the integers as text.\n",
    "\n",
    "We do have to tell h2o that the integer values in the data should be treated as categories and not numeric values. \".asfactor()\" does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    train_h2o_df[feature] = train_h2o_df[feature].asfactor()\n",
    "    test_h2o_df[feature] = test_h2o_df[feature].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = sklearn.preprocessing.OneHotEncoder(categorical_features=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.fit(data)\n",
    "# encoded_train = encoder.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "# rf.fit(encoded_train, labels_train)\n",
    "\n",
    "# Train an h2o drf\n",
    "mushroom_drf = H2ORandomForestEstimator(\n",
    "                model_id=\"mushroom_drf\",\n",
    "                ntrees=500,\n",
    "                stopping_rounds=2,\n",
    "                score_each_iteration=True,\n",
    "                seed=1000000,\n",
    "                balance_classes=False,\n",
    "                histogram_type=\"AUTO\")\n",
    "\n",
    "mushroom_drf.train(x=feature_names,\n",
    "             y='class',\n",
    "             training_frame=train_h2o_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Note that our predict function first transforms the data into the one-hot representation~~\n",
    "\n",
    "Create a predictor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_fn = lambda x: rf.predict_proba(encoder.transform(x))\n",
    "\n",
    "h2o_drf_wrapper = h2o_predict_proba_wrapper(mushroom_drf,feature_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier has perfect accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics.accuracy_score(labels_test, rf.predict(encoder.transform(test)))\n",
    "\n",
    "mushroom_drf.model_performance(test_h2o_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our explainer. The categorical_features parameter lets it know which features are categorical (in this case, all of them). The categorical names parameter gives a string representation of each categorical feature's numerical value, as we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(train ,class_names=class_names, feature_names = feature_names,\n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   categorical_names=categorical_names, kernel_width=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 137\n",
    "exp = explainer.explain_instance(test[i], h2o_drf_wrapper.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now note that the explanations are based not only on features, but on feature-value pairs. For example, we are saying that odor=foul is indicative of a poisonous mushroom. In the context of a categorical feature, odor could take many other values (see below). Since we perturb each categorical feature drawing samples according to the original training distribution, the way to interpret this is: if odor was not foul, on average, this prediction would be 0.24 less 'poisonous'. Let's check if this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odor_idx = feature_names.index('odor')\n",
    "explainer.categorical_names[odor_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.feature_frequencies[odor_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foul_idx = 4\n",
    "non_foul = np.delete(explainer.categorical_names[odor_idx], foul_idx)\n",
    "non_foul_normalized_frequencies = explainer.feature_frequencies[odor_idx].copy()\n",
    "non_foul_normalized_frequencies[foul_idx] = 0\n",
    "non_foul_normalized_frequencies /= non_foul_normalized_frequencies.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Making odor not equal foul')\n",
    "temp = test[i].copy()\n",
    "print('P(poisonous) before:', h2o_drf_wrapper.predict_proba(temp)[0,1])\n",
    "print\n",
    "average_poisonous = 0\n",
    "for idx, (name, frequency) in enumerate(zip(explainer.categorical_names[odor_idx], non_foul_normalized_frequencies)):\n",
    "    if name == 'foul':\n",
    "        continue\n",
    "    temp[odor_idx] = idx\n",
    "    p_poisonous = h2o_drf_wrapper.predict_proba(temp)[0,1]\n",
    "    average_poisonous += p_poisonous * frequency\n",
    "    print('P(poisonous | odor=%s): %.2f' % (name, p_poisonous))\n",
    "print ()\n",
    "print ('P(poisonous | odor != foul) = %.2f' % average_poisonous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in this particular case, the linear model is pretty close: it predicted that on average odor increases the probability of poisonous by 0.26, when in fact it is by 0.23. Notice though that we only changed one feature (odor), when the linear model takes into account perturbations of all the features at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical and Categorical features in the same dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to a dataset that has both numerical and categorical features. Here, the task is to predict whether a person makes over 50K dollars per year. Downloads the data [here](http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\"Hours per week\", \"Country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.genfromtxt('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', delimiter=', ', dtype=str)\n",
    "data = np.genfromtxt('data/adult.csv', delimiter=', ', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:,14]\n",
    "le= sklearn.preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "class_names = le.classes_\n",
    "data = data[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [1,3,5, 6,7,8,9,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_names = {}\n",
    "for feature in categorical_features:\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    le.fit(data[:, feature])\n",
    "    data[:, feature] = le.transform(data[:, feature])\n",
    "    categorical_names[feature] = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to one-hot encode from h2o\n",
    "# encoder = sklearn.preprocessing.OneHotEncoder(categorical_features=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a train test split and convert to pandas and h2o frames\n",
    "\n",
    "np.random.seed(1)\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n",
    "\n",
    "\n",
    "train_h2o_df = h2o.H2OFrame(train)\n",
    "train_h2o_df.set_names(feature_names)\n",
    "train_h2o_df['class'] = h2o.H2OFrame(labels_train)\n",
    "train_h2o_df['class'] = train_h2o_df['class'].asfactor()\n",
    "\n",
    "test_h2o_df = h2o.H2OFrame(test)\n",
    "test_h2o_df.set_names(feature_names)\n",
    "test_h2o_df['class'] = h2o.H2OFrame(labels_test)\n",
    "test_h2o_df['class'] = test_h2o_df['class'].asfactor()\n",
    "\n",
    "print(type(train_h2o_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to one-hot encode from h2o\n",
    "# encoder.fit(data)\n",
    "# encoded_train = encoder.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell h2o which features are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    train_h2o_df[feature] = train_h2o_df[feature].asfactor()\n",
    "    test_h2o_df[feature] = test_h2o_df[feature].asfactor()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~This time, we use gradient boosted trees as the model, using the [xgboost](https://github.com/dmlc/xgboost) package.~~\n",
    "\n",
    "We will use the H2OGradientBoostingEstimator which is h2o's version of gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_gbm = H2OGradientBoostingEstimator(\n",
    "            ntrees=300,\n",
    "            learn_rate=0.1,\n",
    "            max_depth=5,\n",
    "            stopping_tolerance=0.01,\n",
    "            stopping_rounds=2,\n",
    "            score_each_iteration=True,\n",
    "            model_id=\"gbm_v1\",\n",
    "            seed=2000000\n",
    ")\n",
    "adult_gbm.train(feature_names, 'class', training_frame = train_h2o_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics.accuracy_score(labels_test, rf.predict(encoder.transform(test)))\n",
    "\n",
    "adult_gbm.model_performance(test_h2o_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_fn = lambda x: rf.predict_proba(encoder.transform(x)).astype(float)\n",
    "\n",
    "h2o_drf_wrapper = h2o_predict_proba_wrapper(adult_gbm,feature_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(train ,feature_names = feature_names,class_names=class_names,\n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   categorical_names=categorical_names, kernel_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show a few explanations. These are just a mix of the continuous and categorical examples we showed before. For categorical features, the feature contribution is always the same as the linear model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "i = 1653\n",
    "exp = explainer.explain_instance(test[i], h2o_drf_wrapper.predict_proba, num_features=5)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that capital gain has very high weight. This makes sense. Now let's see an example where the person has a capital gain below the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "exp = explainer.explain_instance(test[i], h2o_drf_wrapper.predict_proba, num_features=5)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.shutdown(prompt=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
